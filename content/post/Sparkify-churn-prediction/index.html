---
title: Customer Churn Prediction for Sparkify using PySpark
author: ~
date: '2020-11-09'
slug: Sparkify-churn-prediction
categories: []
tags:
- Machine learning
- Feature engineering
- Spark
subtitle: ''
summary: Use Apache Spark for large-scale data processing to predict customer churn
authors: [Wei Guo]
lastmod: []
featured: no
image:
  caption: ''
  focal_point: ''
projects: []
---



<p>Predicting customer churn is a challenging and common problem for any e-commerce business in which everything depends on the behavior of customers. Customer churn is often defined as the process in which the customers downgrade from premium to free tier or stop using the products or services of a business. Thus, the ability to predict which users are at risk of churning, while there is still time to offer them discounts or other incentives, will greatly help to prevent every custormer-facing business from suffering severe financial losses.</p>
<div id="overview" class="section level2">
<h2>Overview</h2>
<ul>
<li><strong><a href="#data-preparation">Data preparation</a></strong>
<ul>
<li><a href="#data-description">Data description</a></li>
<li><a href="#data-cleaning">Data cleaning</a></li>
</ul></li>
<li><strong><a href="#feature-engineering-and-exploratory-data-analysis">Feature engineering and exploratory data analysis</a></strong>
<ul>
<li><a href="#feature-correlation">Feature correlation</a></li>
</ul></li>
<li><strong><a href="#modeling-and-evaluation">Modeling and evaluation</a></strong>
<ul>
<li><a href="#build-training-pipelines">Build training pipelines</a></li>
<li><a href="#performance-metrics">Performance metrics</a></li>
<li><a href="#hyperparameter-tuning">Hyperparameter tuning</a></li>
<li><a href="#feature-importance">Feature importance</a></li>
</ul></li>
<li><strong><a href="#training-and-testing-on-the-full-dataset">Training and testing on the full dataset</a></strong></li>
<li><strong><a href="#discussion">Discussion</a></strong></li>
</ul>
<p>The datasets in this project are provided by Sparkify, a fictitious digital music service created by Udacity, to resemble the data sets generated by companies such as Spotify or Pandora. Millions of users stream their favorite songs through Sparkify’s platform on a daily basis, either using the free tier that places advertisements between the songs or using the premiumn subscription model which is typically ad-free by paying a monthly flat rate. Users can upgrade, downgrade or cancel their service at any time. Thus, it is crucial to ensure the users love the service. Our goal in this project is to help Spakify identify potential churn users by building and training a binary classifier so as to save the business millions in revenue.</p>
</div>
<div id="data-preparation" class="section level2">
<h2>Data preparation</h2>
<p>Data is generated everytime a user interacts with the service while playing songs, logging out, liking a song with a thumbs up or adding a friend etc. The full dataset collects over 26 million records from 22277 registered users, whereas a smaller subset (mini dataset) contains 286500 records from 225 registered users with a duration of about two months. The model development proces present here is performed on the smaller subset using Python API for Spark, PySpark.</p>
<div id="data-description" class="section level3">
<h3>Data description</h3>
<p>Since there is no documentation provided alongside the datasets, we first have to conduct data exploration to gain a sense of the data. A detailed description of the variables is summarized below.</p>
<center>
<div style="width:100%;">
<table style="width:83%;">
<colgroup>
<col width="13%" />
<col width="13%" />
<col width="55%" />
</colgroup>
<thead>
<tr class="header">
<th>Variable</th>
<th>Data Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>artist</strong></td>
<td>string</td>
<td>artist name</td>
</tr>
<tr class="even">
<td><strong>auth</strong></td>
<td>categorical</td>
<td>authentication level (Logged In, Logged Out, Cancelled, Guest)</td>
</tr>
<tr class="odd">
<td><strong>firstName</strong></td>
<td>string</td>
<td>user’s first name</td>
</tr>
<tr class="even">
<td><strong>gender</strong></td>
<td>categorical</td>
<td>user’s gender (M and F)</td>
</tr>
<tr class="odd">
<td><strong>itemInSession</strong></td>
<td>int</td>
<td>log count in a given session</td>
</tr>
<tr class="even">
<td><strong>lastName</strong></td>
<td>string</td>
<td>user’s last name</td>
</tr>
<tr class="odd">
<td><strong>length</strong></td>
<td>double</td>
<td>song’s length in seconds</td>
</tr>
<tr class="even">
<td><strong>level</strong></td>
<td>string</td>
<td>subscription level (free and paid)</td>
</tr>
<tr class="odd">
<td><strong>location</strong></td>
<td>string</td>
<td>user’s location</td>
</tr>
<tr class="even">
<td><strong>method</strong></td>
<td>categorical</td>
<td>http request method (GET and PUT)</td>
</tr>
<tr class="odd">
<td><strong>page</strong></td>
<td>categorical</td>
<td>type of interaction (NextSong, Home, Cancellation Confirmation, etc.)</td>
</tr>
<tr class="even">
<td><strong>registration</strong></td>
<td>int</td>
<td>user’s registration timestamp</td>
</tr>
<tr class="odd">
<td><strong>sessionId</strong></td>
<td>int</td>
<td>session to which the log belongs to</td>
</tr>
<tr class="even">
<td><strong>song</strong></td>
<td>string</td>
<td>song name currently being played</td>
</tr>
<tr class="odd">
<td><strong>status</strong></td>
<td>categorical</td>
<td>http status code (200, 307 and 404)</td>
</tr>
<tr class="even">
<td><strong>ts</strong></td>
<td>int</td>
<td>timestamp of a given log</td>
</tr>
<tr class="odd">
<td><strong>userAgent</strong></td>
<td>string</td>
<td>agent used by the user to access the streaming service</td>
</tr>
<tr class="even">
<td><strong>userId</strong></td>
<td>string</td>
<td>user identifier</td>
</tr>
</tbody>
</table>
</div>
</center>
</div>
<div id="data-cleaning" class="section level3">
<h3>Data cleaning</h3>
<p>After checking null values, we notice variables with null values can be divided into two groups, i.e.:</p>
<ul>
<li>Group 1 (user-related): <strong>firstName</strong>, <strong>gender</strong>, <strong>lastName</strong>, <strong>location</strong>, <strong>registration</strong> and <strong>userAgent</strong></li>
<li>Group 2 (song-related): <strong>artist</strong>, <strong>length</strong> and <strong>song</strong></li>
</ul>
<p>It seems variables in each group are tied in a certain way with nulls. With more data wrangling, we find:</p>
<ul>
<li>When <strong>auth</strong> is <em>LoggedOut</em>, variable values in both groups are nulls and values in <strong>userId</strong> are empty strings.</li>
<li>All the rows with missing values in the <strong>userId</strong> column and all other user-related columns correspond to <em>Logged Out</em> or <em>Guest</em> authentication levels.</li>
<li>Variable values in Group 2 are nulls whenever <strong>page</strong> is not <em>NextSong</em>, which makes sense since these variables are song-related.</li>
</ul>
<p>Based on our observations above, we only need to remove the records when the <strong>userId</strong> is an empty string. Additionally, we replace the <strong>location</strong> column by the corresponding state name to simplify the location-related analysis. If the location links to multiple states, we select the first state to represent the user’s location.</p>
<pre class="python"><code>def clean_data(df):
    &quot;&quot;&quot;Clean a Sparkify dataset in the form of the Spark dataframe 
    
    Args:
    df: a Sparkify dataset
    
    Returns:
    df: a preprocessed Sparkify dataset
    &quot;&quot;&quot;
    # remove user id with empty strings
    df = df.filter(col(&#39;userId&#39;) != &#39;&#39;)
    
    # convert &#39;registration&#39; and &#39;ts&#39; to date format
    df = df \
        .withColumn(&#39;registrationTime&#39;, to_timestamp(col(&#39;registration&#39;)/1000)) \
        .withColumn(&#39;time&#39;, to_timestamp(col(&#39;ts&#39;)/1000)) 
    
    # replace location with first listed state 
    state_udf = udf(lambda x: x.split(&#39;, &#39;)[1].split(&#39;-&#39;)[0])
    df = df.withColumn(&#39;location&#39;, state_udf(&#39;location&#39;))
        
    return df</code></pre>
</div>
</div>
<div id="feature-engineering-and-exploratory-data-analysis" class="section level2">
<h2>Feature engineering and exploratory data analysis</h2>
<p>We use the <em>Cancellation Confirmation</em> events of the <strong>page</strong> column to define the customer churn.</p>
<pre class="python"><code># create a window partitioned by &#39;userId&#39;
userWindow = Window.partitionBy(&#39;userId&#39;).orderBy(&#39;ts&#39;).rangeBetween(Window.unboundedPreceding,Window.unboundedFollowing)

# label churned users to be 1 and unchurned users to be 0 
df = df \
    .withColumn(&#39;Churn&#39;, when(col(&#39;page&#39;)==&#39;Cancellation Confirmation&#39;, 1).otherwise(0)) \
    .withColumn(&#39;Churn&#39;, max(&#39;Churn&#39;).over(userWindow))</code></pre>
<p>By this definition, there are 52 churned users in a total of 225 users in the dataset. That is about 23.1% churn rate. Next, we investigate the following factors of interest for their impact on churn:</p>
<ul>
<li><strong>gender</strong></li>
<li><strong>location</strong>: user’s state</li>
<li><strong>latestLevel</strong>: user’s latest subscription level for each user</li>
<li><strong>registDuration</strong>: user’s registration duration (in days)</li>
<li><strong>avgSessionDuration</strong>: user’s average session duration (in hours)</li>
<li><strong>avgDailySongs</strong>, <strong>avgDailyThumbsUp</strong>, <strong>avgDailyThumbsDown</strong>, <strong>avgDailyUpgrade</strong>, <strong>avgDailyDowngrade</strong>, <strong>avgDailyAddFriend</strong>, <strong>avgDailyAddPlaylist</strong>, <strong>avgDailyAdvert</strong>, <strong>avgDailyHelp</strong>, <strong>avgDailyError</strong>: user’s average daily songs played, thumbs up given, thumbs down given, upgrades, downgrades, friends added, songs added to playlist, advertisements played, help page visits, errors encountered, respectively</li>
</ul>
<p>In this list, gender and location represent a user’s demographic information. We notice Spakify’s users reside across 41 states. If we one hot encode the location feature, we will add 40 binary columns to a dataset with only 225 samples, which may potentially cause the curse of dimensionality. To address this issue, we replace each state by its corresponding geographical division (data source: <a href="https://www2.census.gov/geo/pdfs/maps-data/maps/reference/us_regdiv.pdf" class="uri">https://www2.census.gov/geo/pdfs/maps-data/maps/reference/us_regdiv.pdf</a>) so that 41 states are assigned to 9 divisions.</p>
<pre class="python"><code>region_df = spark.read.csv(&#39;us_regions.csv&#39;, header=True)

# find the user&#39;s geographical division based on the location
division_udf = udf(lambda x: &#39;location&#39; + x.replace(&#39; &#39;, &#39;&#39;))

location_df = df.select([&#39;userId&#39;, &#39;location&#39;]).dropDuplicates([&#39;userId&#39;])
location_df = location_df.join(region_df, location_df[&#39;location&#39;]==region_df[&quot;State Code&quot;], how=&#39;left&#39;) \
            .select([&#39;userId&#39;, col(&quot;Division&quot;).alias(&quot;location&quot;)]) \
            .withColumn(&#39;location&#39;, division_udf(&#39;location&#39;))

# one hot encode the &#39;location&#39; column by pivoting it 
location_df = location_df.groupBy(&#39;userId&#39;).pivot(&#39;location&#39;).agg(count(&#39;location&#39;).cast(IntegerType())).fillna(0)
# remove the last column (or any one) to keep the binary columns independent
location_df = location_df.drop(location_df.columns[-1]) </code></pre>
<p>Now we engineer the rest of the features, all of which are related to a user’s behavior.</p>
<pre class="python"><code># find the latest level of each user
df = df.withColumn(&#39;latestLevel&#39;, last(col(&#39;level&#39;)).over(userWindow))

# calculate the duration between registration to last activity (in days)
regist_duration_df = df.groupBy(&#39;userId&#39;) \
                    .agg(((last(col(&#39;ts&#39;))-last(col(&#39;registration&#39;)))/1000/3600/24).alias(&#39;registDuration&#39;))
                    
# compute average session duration (in hours)
avg_session_duration_df = df \
    .groupby([&#39;userId&#39;, &#39;sessionId&#39;]).agg(min(col(&#39;ts&#39;)).alias(&#39;session_start&#39;), max(col(&#39;ts&#39;)).alias(&#39;session_end&#39;))\
    .groupby(&#39;userId&#39;).agg(avg((col(&#39;session_end&#39;) - col(&#39;session_start&#39;))/1000/3600).alias(&#39;avgSessionDuration&#39;))

# define the default start and end of the observation period
obs_start_default = df.select(min(col(&#39;ts&#39;))).collect()[0][0]
obs_end_default = df.select(max(col(&#39;ts&#39;))).collect()[0][0]  

# compute the observation period
df = df \
    .withColumn(&#39;obs_start&#39;, when(col(&#39;registration&#39;) &gt; obs_start_default, first(col(&#39;ts&#39;)).over(userWindow)) \
                .otherwise(obs_start_default)) \
    .withColumn(&#39;end_state&#39;, last(col(&#39;page&#39;)).over(userWindow)) \
    .withColumn(&#39;obs_end&#39;, when(col(&#39;end_state&#39;) == &#39;Cancellation Confirmation&#39;, last(col(&#39;ts&#39;)).over(userWindow)) \
                .otherwise(obs_end_default)) \
    .withColumn(&#39;obsDays&#39;, (col(&#39;obs_end&#39;) - col(&#39;obs_start&#39;))/1000/3600/24)

# aggregate activity statistics
user_df = df.groupBy(&#39;userId&#39;) \
            .agg(first(col(&#39;Churn&#39;)).alias(&#39;Churn&#39;), \
                 first(when(col(&#39;gender&#39;) == &#39;M&#39;, 1).otherwise(0)).alias(&#39;gender&#39;), \
                 first(col(&#39;location&#39;)).alias(&#39;location&#39;), \
                 first(when(col(&#39;latestLevel&#39;) == &#39;paid&#39;, 1).otherwise(0)).alias(&#39;latestLevel&#39;), \
                 first(col(&#39;registDuration&#39;)).alias(&#39;registDuration&#39;), \
                 first(col(&#39;obsDays&#39;)).alias(&#39;obsDays&#39;), \
                 sum(when(col(&#39;page&#39;) == &#39;NextSong&#39;, 1).otherwise(0)).alias(&#39;nSongs&#39;), \
                 sum(when(col(&#39;page&#39;) == &#39;Thumbs Up&#39;, 1).otherwise(0)).alias(&#39;nThumbsUp&#39;), \
                 sum(when(col(&#39;page&#39;) == &#39;Thumbs Down&#39;, 1).otherwise(0)).alias(&#39;nThumbsDown&#39;), \
                 sum(when((col(&#39;page&#39;) == &#39;Upgrade&#39;) | (col(&#39;page&#39;) == &#39;Submit Upgrade&#39;), 1).otherwise(0)).alias(&#39;nUpgrade&#39;), \
                 sum(when((col(&#39;page&#39;) == &#39;Downgrade&#39;) | (col(&#39;page&#39;) == &#39;Submit Downgrade&#39;), 1).otherwise(0)).alias(&#39;nDowngrade&#39;), \
                 sum(when(col(&#39;page&#39;) == &#39;Add Friend&#39;, 1).otherwise(0)).alias(&#39;nAddFriend&#39;), \
                 sum(when(col(&#39;page&#39;) == &#39;Add to Playlist&#39;, 1).otherwise(0)).alias(&quot;nAddPlaylist&quot;), \
                 sum(when(col(&#39;page&#39;) == &#39;Roll Advert&#39;, 1).otherwise(0)).alias(&#39;nAdvert&#39;), \
                 sum(when((col(&#39;page&#39;) == &#39;Help&#39;), 1).otherwise(0)).alias(&#39;nHelp&#39;), \
                 sum(when((col(&#39;page&#39;) == &#39;Error&#39;), 1).otherwise(0)).alias(&#39;nError&#39;)) \
            .join(location_df, on=&#39;userId&#39;) \
            .join(regist_duration_df, on=&#39;userId&#39;) \
            .join(avg_session_duration_df, on=&#39;userId&#39;)

user_df = user_df \
        .withColumn(&#39;avgDailySongs&#39;, col(&#39;nSongs&#39;) / col(&#39;obsDays&#39;)) \
        .withColumn(&#39;avgDailyThumbsUp&#39;, col(&#39;nThumbsUp&#39;) / col(&#39;obsDays&#39;)) \
        .withColumn(&#39;avgDailyThumbsDown&#39;, col(&#39;nThumbsDown&#39;) / col(&#39;obsDays&#39;)) \
        .withColumn(&#39;avgDailyUpgrade&#39;, col(&#39;nUpgrade&#39;) / col(&#39;obsDays&#39;)) \
        .withColumn(&#39;avgDailyDowngrade&#39;, col(&#39;nDowngrade&#39;) / col(&#39;obsDays&#39;)) \
        .withColumn(&#39;avgDailyAddFriend&#39;, col(&#39;nAddFriend&#39;) / col(&#39;obsDays&#39;)) \
        .withColumn(&#39;avgDailyAddPlaylist&#39;, col(&#39;nAddPlaylist&#39;) / col(&#39;obsDays&#39;)) \
        .withColumn(&#39;avgDailyAdvert&#39;, col(&#39;nAdvert&#39;) / col(&#39;obsDays&#39;)) \
        .withColumn(&#39;avgDailyHelp&#39;, col(&#39;nHelp&#39;) / col(&#39;obsDays&#39;)) \
        .withColumn(&#39;avgDailyError&#39;, col(&#39;nError&#39;) / col(&#39;obsDays&#39;)) \
        .drop(&#39;userId&#39;, &#39;obsDays&#39;, &#39;nSongs&#39;, &#39;nThumbsUp&#39;, &#39;nThumbsDown&#39;, &#39;nUpgrade&#39;, &#39;nDowngrade&#39;, \
              &#39;nAddFriend&#39;, &#39;nAddPlaylist&#39;, &#39;nAdvert&#39;, &#39;nHelp&#39;, &#39;nError&#39;)</code></pre>
<p><img src="gender_latest_level_counts.png" style="width:80.0%" /> <img src="location_counts.png" style="width:80.0%" /></p>
<p>An overall look at the first three categorical feaures indicates:</p>
<ul>
<li>The churn rates are only slightly higher for male users and free users</li>
<li>The churn rates differ greatly among divisions. For example, New England area has zero churn rate while there are as many churned users as unchurned ones in east south central area.</li>
</ul>
<div class="figure">
<img src="numerical_features_hist.png" style="width:100.0%" />

</div>
<p>By comparing the numerical feature distributions between churned users and unchurned ones, we have the following observations:</p>
<ul>
<li>Churned users have a shorter registration duration on average.</li>
<li>Churned users give more thumbs down, need more help, and have to watch more advertisements per day.</li>
<li>There is no significant difference between churned and unchurned users in terms of average session duration.</li>
</ul>
<div id="feature-correlation" class="section level3">
<h3>Feature correlation</h3>
<p>Before we start modeling, we also need to understand the correlation between the response variable and the features as well as the correlation between the features. The <strong>Correlation</strong> function in the <strong>ml.stat</strong> subpackage requires a column of type Vector. Thus, we need to convert the columns into a vector column first and then apply the correlation.</p>
<pre class="python"><code>vec_col = &#39;corr_features&#39;
   
# assemble all vector columns into one vector column
assembler = VectorAssembler(inputCols=user_df.columns, outputCol=vec_col)
corr_df = assembler.transform(user_df).select(vec_col)

# compute the correlation between &#39;churn&#39; and every feature and the correlation between each pair of features
corr_mat = Correlation.corr(corr_df, vec_col)
# convert the corrlation matrix to a pandas dataframe with column names
corr_values = corr_mat.collect()[0][0].values
corr_mat_pd = pd.DataFrame(corr_values.reshape(-1, len(user_df.columns)), \
                           index=user_df.columns, columns=user_df.columns)

# plot feature correlations
sns.heatmap(corr_mat_pd, vmin=-1, vmax=1, cmap=sns.diverging_palette(240, 20, as_cmap=True), 
            linewidths=.5, cbar_kws={&quot;shrink&quot;: .5}, square=True)</code></pre>
<div class="figure">
<img src="feature_correlations.png" style="width:80.0%" />

</div>
<p>From the heatmap above, we find</p>
<ul>
<li><p>Many page event activities, such as <strong>avgDailySongs</strong>, <strong>avgDailyThumbsUp</strong>, <strong>avgDailyDowngrade</strong>, <strong>avgDailyAddFriend</strong>, <strong>avgDailyAddPlaylist</strong> and <strong>avgDailyHelp</strong>, are highly correlated. To avoid multicollinearity in the data, we only retain <strong>avgDailyAddPlaylist</strong> here (as it has the highest correlation with <strong>Churn</strong>). On the other hand, location features are weakly correlated with each other.</p></li>
<li><p><strong>Churn</strong> doesn not appear to have strong correlations with any features. <strong>registDuration</strong> is negatively correlated with <strong>Churn</strong> while <strong>avgDailyAdvert</strong> has the highest positive correlation with <strong>Churn</strong>. It also shows that <strong>gender</strong>, <strong>latestLevel</strong>, <strong>avgSessionDuration</strong> and many location features all have very low correlation with <strong>Churn</strong>. This is consistent with our previous findings through direct visualization. However, the Pearson correlation assumes a linear relationship, and a low correlation simply means that there is no linear relationship between these features and the response variable. The data might still have a nonlinear relationship and should be included in the modeling for now.</p></li>
</ul>
<p>The code that automates the removal of highly correlated features is shown below:</p>
<pre class="python"><code># construct an adjacency matrix where high correlation (&gt; 0.85) is labeled as 1, otherwise 0
is_high_corr = np.abs(corr_mat_pd.values) &gt; 0.85
adj_mat = csr_matrix(is_high_corr.astype(int) - np.identity(len(user_df.columns)))

# find groups of highly correlated features by finding the connected components in the adjacency matrix
_, corr_labels = connected_components(csgraph=adj_mat, directed=False)
unique, unique_counts = np.unique(corr_labels, return_counts=True)
# get groups with size &gt; 1
high_corr_labels = unique[unique_counts &gt; 1]

# if there is at least one group of highly correlated features
if len(high_corr_labels) &gt; 0:
    # map the label indices of highly correlated features to their column names
    print(&#39;Highly correlated features include:&#39;)
    high_corr_col_dict = {}
    for high_corr_label in high_corr_labels:
        high_corr_col_dict[high_corr_label] = [col_name for corr_label, col_name in zip(corr_labels, user_df.columns) 
                                               if corr_label == high_corr_label]
        print(high_corr_col_dict[high_corr_label])
        
    print(&#39;\nFeatures to keep:&#39;)
    cols_to_drop = []
    for col_name_list in high_corr_col_dict.values(): 
        # keep the feature that has the highest correlation with &#39;Churn&#39;
        col_to_keep = corr_mat_pd.loc[col_name_list,&#39;Churn&#39;].idxmax()
        print(col_to_keep)
        # remove the other features to avoid multicollinearity 
        col_name_list.remove(col_to_keep)
        corr_mat_pd.drop(index=col_name_list, columns=col_name_list, inplace=True)
        cols_to_drop.extend(col_name_list)
        
model_df = user_df.drop(*cols_to_drop)</code></pre>
<p>Now a total of 10 binary features and 7 numerical features are ready for modeling as inputs.</p>
</div>
</div>
<div id="modeling-and-evaluation" class="section level2">
<h2>Modeling and evaluation</h2>
<p>We first randomly split the entire dataset into a training and a test set in 8:2 ratio using stratified sampling based on <strong>Churn</strong>.</p>
<pre class="python"><code># split the data into train, validation and test sets using stratified sampling based on &#39;Churn&#39;
train = model_df.sampleBy(&#39;Churn&#39;, fractions={0: 0.8, 1: 0.8}, seed=2020)
test = model_df.subtract(train_val)</code></pre>
<p>Three binary classifiers supported in Spark are selected as candidate models: logistic regression, random forest classifier and gradient-boosted tree classifier. Both random forest and gradient-boosted techniques employ an ensemble of decision trees (base estimators) to improve the predictive capability over a single estimator. The difference is random forest builds several estimators independently and then average their predictions, while gradient-boosted tree buils base estimators sequentially to generate a powerful ensemble.</p>
<div id="build-training-pipelines" class="section level3">
<h3>Build training pipelines</h3>
<p>For each binary classifier, we build a pipeline to facilitate the machine learning workflow and prevent data leakage.</p>
<pre class="python"><code>num_cols = [field.name for field in model_df.schema.fields if field.dataType != IntegerType()]
bin_cols = [col for col in model_df.columns if col not in num_cols + [&#39;Churn&#39;]]

# assemble numerical columns to a single vector column 
num_assembler = VectorAssembler(inputCols=num_cols, outputCol=&#39;num_features&#39;)
    
# scale each numberical feature within the range [0,1] 
scaler = MinMaxScaler(inputCol=&#39;num_features&#39;, outputCol=&#39;scaled_features&#39;)
    
# assemble all vector columns into one vector column
assembler = VectorAssembler(inputCols= bin_cols + [&#39;scaled_features&#39;], outputCol=&#39;features&#39;)

# logistic regression
lr = LogisticRegression(featuresCol=&#39;features&#39;, labelCol=&#39;Churn&#39;)
pipeline_lr = Pipeline(stages=[num_assembler, scaler, assembler, lr])

# random forest classifier
rf = RandomForestClassifier(featuresCol=&#39;features&#39;, labelCol=&#39;Churn&#39;, seed=2020)
pipeline_rf = Pipeline(stages=[num_assembler, scaler, assembler, rf])

# gradient-boosted tree classifier
gb = GBTClassifier(featuresCol=&#39;features&#39;, labelCol=&#39;Churn&#39;, seed=2020)
pipeline_gb = Pipeline(stages=[num_assembler, scaler, assembler, gb])</code></pre>
</div>
<div id="performance-metrics" class="section level3">
<h3>Performance metrics</h3>
<p>We choose to use the <strong>f1 score</strong> as a performance metric since our dataset is imbalanced. The f1 score is the harmonic mean of the precision and recall. Spark’s <strong>MulticlassClassificationEvaluator</strong> module in the <strong>ml.evaluation</strong> subpackage only provides weighted f1 score, precision and recall metrics. For an imbalanced dataset dominated by the negative class, the weighted metrics will inflate the corresponding metrics from the positive class which we are more interested in. For this reason, we employ the <strong>MulticlassMetrics</strong> module from Spark’s <strong>mllib.evaluation</strong> subpackage to calculate these metrics for each class. Another metric we look at is the <strong>area under the precision-recall curve (AUC-PR)</strong>. For binary classifier evaluation, precision-recall curves are more useful in practice than the receiver operating characteristic (ROC) curves for problems where the “positive” class is more of interest than the negative class. See: <a href="http://pages.cs.wisc.edu/~jdavis/davisgoadrichcamera2.pdf" class="uri">http://pages.cs.wisc.edu/~jdavis/davisgoadrichcamera2.pdf</a></p>
<pre class="python"><code>def print_metrics(pred, label=&#39;Churn&#39;):
    &quot;&quot;&quot;Print evaluation metrics on a test set
    
    Args:
    pred: (spark dataframe) a test set 
    
    Returns:
    summary: (pandas dataframe) a summary of evaluation metrics
    &quot;&quot;&quot;
    eval_metrics = {}

    # compute area under PR curve
    evaluator = BinaryClassificationEvaluator(labelCol=label)
    auc_pr = evaluator.evaluate(pred, {evaluator.metricName:&#39;areaUnderPR&#39;})

    # compute precision, recall and f1 score
    predictionAndLabels = pred.select(&#39;prediction&#39;, label)
    # both &#39;prediction&#39; and label in predictionAndLabels need to be cast to float type and 
    # map to tuple before calling &#39;MulticlassMetrics&#39;
    metrics = MulticlassMetrics(predictionAndLabels.rdd.map(lambda x: tuple(map(float, x))))

    # get overall statistics
    eval_metrics[&#39;overall&#39;] = [metrics.weightedPrecision, metrics.weightedRecall, \
                               metrics.weightedFMeasure(), auc_pr]
                               
    # get statistics by class
    classes = [0.0, 1.0]
    for cls in classes:
        eval_metrics[&#39;class &#39; + str(int(cls))] = [metrics.precision(cls), metrics.recall(cls), \
                                                  metrics.fMeasure(cls), &#39;&#39;]

    # convert to a pandas dataframe for display
    summary = pd.DataFrame.from_dict(eval_metrics, orient=&#39;index&#39;, \
                                     columns=[&#39;precision&#39;, &#39;recall&#39;, &#39;f1 score&#39;, &#39;AUC-PR&#39;])   
    
    return summary</code></pre>
</div>
<div id="hyperparameter-tuning" class="section level3">
<h3>Hyperparameter tuning</h3>
<p>The search spaces of hyperparameters for the three models are listed below:</p>
<ul>
<li><strong>Logistic Regression</strong>
<ul>
<li><em>maxIter</em> (maximum number of iterations, default=100): [10, 30]</li>
<li><em>regParam</em> (regularization parameter, default=0.0): [0.0, 0.1]</li>
<li><em>elasticNetParam</em> (mixing parameter - 0 for L2 penalty, 1 for L1 penalty, default=0.0: [0.0, 0.5]</li>
</ul></li>
<li><strong>Random Forest Classifier</strong>
<ul>
<li><em>maxDepth</em> (maximum tree depth, default=5): [4, 5]</li>
<li><em>numTrees</em> (number of trees, default=20): [20, 100, 200]</li>
</ul></li>
<li><strong>Gradient-Boosted Tree Classifier</strong>
<ul>
<li><em>maxDepth</em> (maximum tree depth, default=5): [4, 5]</li>
<li><em>maxIter</em> (maximum number of iterations, default=20): [20, 100]</li>
</ul></li>
</ul>
<p>We perform a grid search with 4-fold cross validation to test the performance of these hyperparameter combinations measured by <strong>AUC-PR</strong>. An advantage of using AUC-PR is that one does not need to worry about optimizing the probability threshold that splits the model outputs into positive and negative predictions, as it summarizes the model performance over all possible thresholds.</p>
<pre class="python"><code># set hyperparameters for tuning
paramGrid = ParamGridBuilder() \
            .addGrid(rf.numTrees, [20, 100, 200]) \
            .addGrid(rf.maxDepth, [4, 5]) \
            .build()  

# grid search with cross validation    
crossval_rf = CrossValidator(estimator = pipeline_rf,
                             estimatorParamMaps = paramGrid,
                             evaluator = BinaryClassificationEvaluator(labelCol=&#39;Churn&#39;, metricName=&#39;areaUnderPR&#39;),
                             numFolds = 4)

cv_rf = crossval_rf.fit(train)
test_prediction = cv_rf.transform(test)
print_metrics(test_prediction)</code></pre>
<p>The results obtained on the test set from the best model of each classifier are summarized in the table below:</p>
<div style="width:100%;">
<table style="width:100%;">
<colgroup>
<col width="22%" />
<col width="18%" />
<col width="8%" />
<col width="8%" />
<col width="8%" />
<col width="8%" />
<col width="8%" />
<col width="8%" />
<col width="8%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Classifier</th>
<th align="left">Parameters</th>
<th align="center">Precision</th>
<th align="center"></th>
<th align="center">Recall</th>
<th align="center"></th>
<th align="center">F1 score</th>
<th align="center"></th>
<th align="center">AUC-PR</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"></td>
<td align="left"></td>
<td align="center">Overall</td>
<td align="center">Churned</td>
<td align="center">Overall</td>
<td align="center">Churned</td>
<td align="center">Overall</td>
<td align="center">Churned</td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="left">Logistic regression</td>
<td align="left">maxIter=10, regParam=0.1, elasticNetParam=0.5</td>
<td align="center">0.85</td>
<td align="center">1.00</td>
<td align="center">0.82</td>
<td align="center">0.20</td>
<td align="center">0.77</td>
<td align="center">0.33</td>
<td align="center">0.72</td>
</tr>
<tr class="odd">
<td align="left">Random forest classifier</td>
<td align="left">maxDepth=4, numTrees=100</td>
<td align="center">0.86</td>
<td align="center"><strong>0.75</strong></td>
<td align="center">0.86</td>
<td align="center"><strong>0.60</strong></td>
<td align="center"><strong>0.86</strong></td>
<td align="center"><strong>0.67</strong></td>
<td align="center"><strong>0.77</strong></td>
</tr>
<tr class="even">
<td align="left">Gradient-boosted tree classifier</td>
<td align="left">maxDepth=5, maxIter=100</td>
<td align="center">0.77</td>
<td align="center">0.43</td>
<td align="center">0.73</td>
<td align="center">0.60</td>
<td align="center">0.74</td>
<td align="center">0.50</td>
<td align="center">0.65</td>
</tr>
</tbody>
</table>
</div>
<p>It shows that the random forest classifier outperforms both the logistic regression model and the gradient-boosted tree classifier in almost all categories with an overall f1 score of 0.86 and AUC-PR of 0.77. As aforementioned, the features and the response variable do not follow a linear relationship, which might suggest that a non-linear model will be more suited to the data. Based on the solid performance on the mini dataset, we decide to use random forest classifier for future training.</p>
</div>
<div id="feature-importance" class="section level3">
<h3>Feature importance</h3>
<p>To gain a better understanding of the feature attributes to the model in predicting user churn, we extract feature importances from the trained model.</p>
<pre class="python"><code>features = bin_cols + num_cols
importances = list(cv_rf.bestModel.stages[-1].featureImportances)
feat_imp_pd = pd.DataFrame({&#39;feature&#39;: features, &#39;importance&#39;: importances}).sort_values(&#39;importance&#39;, ascending = False)

# plot feature importance
sns.barplot(data=feat_imp_pd.head(15), y=&quot;feature&quot;, x=&quot;importance&quot;, palette=&#39;Blues_r&#39;, zorder=2)
plt.grid(axis=&#39;x&#39;, linestyle=&#39;--&#39;, zorder=0)
plt.ylabel(&#39;&#39;);</code></pre>
<div class="figure">
<img src="feature_importances.png" style="width:80.0%" />

</div>
<p>It can be seen that the most important feature for identifying churned users is the registration duration which makes sense since it directly reflects a user’s willness to stay with the service. Another influential feature that plays a major role in churn prediction is avarage daily advertisements. Intuitively, the more advertisements a user has to watch, the more likely he/she is dissatisfied with the service. Besides, average upgrades and thumbs down given per day that represent a user’s strong inclination also help churn detection.</p>
<p>The result also shows that gender, latest subscription level and each location feature contribute little to predicting churned users. Nevertheless, we notice that the churn rates of top location features, sch as those of New England and east south central areas are far different from the average churn rate.</p>
</div>
</div>
<div id="training-and-testing-on-the-full-dataset" class="section level2">
<h2>Training and testing on the full dataset</h2>
<p>We apply the scalable machine learning pipeline we have developed here to the full Sparkify dataset. The task is carried out on an Amazon Elastic MapReduce (EMR) cluster. With more training data, the search space of maximum tree depth and number of trees for the random forest classifier is adapted to [8, 10] and [100, 200], respectively.</p>
<p>A key insight from the study on the mini dataset is that the attributions of <strong>gender</strong>, <strong>latest subscription level</strong> and <strong>location</strong> features are negligible to model prediction. We test this finding on the full dataset, and the model trained without these features achives practically the same performance as the model including these features with a reduction of ~40% training time. The results evaluated on the test set of the full dataset from the best random forest model are given below:</p>
<div style="width:100%;">
<table style="width:100%;">
<colgroup>
<col width="22%" />
<col width="17%" />
<col width="8%" />
<col width="8%" />
<col width="8%" />
<col width="8%" />
<col width="8%" />
<col width="8%" />
<col width="8%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Classifier</th>
<th align="left">Parameters</th>
<th align="center">Precision</th>
<th align="center"></th>
<th align="center">Recall</th>
<th align="center"></th>
<th align="center">F1 score</th>
<th align="center"></th>
<th align="center">AUC-PR</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"></td>
<td align="left"></td>
<td align="center">Overall</td>
<td align="center">Churned</td>
<td align="center">Overall</td>
<td align="center">Churned</td>
<td align="center">Overall</td>
<td align="center">Churned</td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="left">Random forest classifier</td>
<td align="left">maxDepth=10, numTrees=100</td>
<td align="center">0.92</td>
<td align="center"><strong>0.93</strong></td>
<td align="center">0.91</td>
<td align="center"><strong>0.64</strong></td>
<td align="center"><strong>0.91</strong></td>
<td align="center"><strong>0.76</strong></td>
<td align="center"><strong>0.89</strong></td>
</tr>
</tbody>
</table>
</div>
<p>We can see that there are sizeable performance gains in every metric category due to more training data. At a default probability threshold of 0.5, our model is currently able to identify 64% of user churn and 7% identified as churned users acutally satisfy with the service. If deployed, the trained model could be used to predict users inclined to churn in advance, and we can provide them with special offers in the hope of keeping them from deleting their Sparkify accounts. If we want to target more users prone to churning, we can lower the probability threshold, and a high AUC-PR score of 0.89 will allow us to maintain a high precision that there will not be many more users with no intent of churning mistakenly targeted.</p>
</div>
<div id="discussion" class="section level2">
<h2>Discussion</h2>
<p>In this project, since the dataset is relatively clean, the most critical step in the model development process is feature engineering. It has been proved most of the engineered features based on users’ behaviors play vital roles in churn prediction rather than users’ demographical information (e.g., gender and location). This also suggests that we can take more actions towards users’ behaviors aside from providing coupons. For example, our study shows churned users receive more daily advertisements on average. Accordingly, we should reduce the amount of advertisements for those who are likely to churn. Meanwhile, it is necessary to conduct A/B tests to statistically assess benefits vs. costs by such actions.</p>
<p>For further imporvements, we should experiment with more engineered features that capture the insights about users’ activity patterns and trends. In addtion, we can apply principle component analysis (PCA) to input features before feeding them to training. PCA allows us to reduce the number of features and decorrelate them, which often helps to improve the performance of the model downstream.</p>
</div>
